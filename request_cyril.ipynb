{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>final_proj-cdmvansc</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=final_proj-cdmvansc>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('final_proj-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '4g')\n",
    "conf.set('spark.executor.instances', '10')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/datasets/project/istdaten/*/*/*', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we rename the columns in English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = 'TripDate string, TripId string, OperatorId string, OperatorAbbrv string, OperatorName string, ProductId string, LineId string, LineType string, UmlaufId string, TransportType string, AdditionalTrip boolean, FailedTrip boolean, BPUIC string, StopName string, ArrivalTimeScheduled string, ArrivalTimeActual string, ArrivalTimeActualStatus string,     DepartureTimeScheduled string, DepartureTimeActual string, DepartureTimeActualStatus string, SkipStation boolean'\n",
    "columns = list(map(lambda x: x.split()[0],columns.split(',')))\n",
    "\n",
    "for old, new in zip(df.columns, columns):\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the quality of a transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions: \n",
    "\n",
    "   * everytime when making a transfer in a station, the traveler needs one minute for actually changing transport.\n",
    "   * even though a train departs late all the time in a specific station, the trip planner will never use the fact that it does so, so we will only take into consideration the early departures and the correct ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea: \n",
    "\n",
    "The idea behind computing the quality of a specific transfer given the *expected arrival hour* in the station and the *expected departure hour* from that same station, and some *extra information* regarding the trip before the transfer and the one after the transfer:\n",
    "\n",
    "   * First, we compute the **discrete distribution of arrival delays $\\mathcal{D}_a$** in that station, given the information of the trip before the transfer.\n",
    "   * Then, we compute the **discrete distribution of negative departure delays $\\mathcal{D}_d$** in that station, given the information of the trip after the transfer.\n",
    "   * Next, we compute the probability of successfully realizing the transfer, by computing a convolution between the two given distributions. Therefore, assuming that the time of transfer is $k$ minutes, then we would simply compute:\n",
    "      \n",
    "      $\\sum\\limits_{t_a }\\Pr[\\mathcal{D}_a = t_a] \\cdot \\Pr[\\mathcal{D}_d = k-1+t_a]$,\n",
    "      \n",
    "       where we have taken into consideration the minute needed by the traveler for changing the transport. \n",
    "       \n",
    "---\n",
    "       \n",
    "Therefore, we first need to decide what are the features which will decide the distributions of the delays. For that, we will use a **Decision Tree Regressor**, selecting several features which might be important from the data, and the target label will be the delay for each datapoint, expressed in seconds. Then, we will train the regressor on both departures and arrivals data, and will look into which are the most important features in each case, for making a good prediction of the delay time. \n",
    "\n",
    "We have to emphasize that we considered this method, because of the way that Decision Trees decide which are the most important feature, i.e. the one which have the most variance of delays between the different values for the specific feature. \n",
    "\n",
    "After constructing the Decision Tree and deciding which are the most important features, we will construct the distributions of the delays from the **actual data**, by grouping the datapoints with the same value for the decisive features, and making the distribution of delays for each group.\n",
    "\n",
    "We decided to use the actual data instead of modelling the distribution of delays using a fixed distribution family (e.g. Log-normal or Gamma distributions), because we consider that the actual data is more relevant, then considering just an estimator or to assume that it follows a distribution in a family of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing the Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in constructing the Decision Tree Regressor is to construct some potential important features from the given data, and also to compute the delays for each datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, to_timestamp\n",
    "\n",
    "DATE_FORMAT_SCHEDULED = 'dd.MM.yyyy HH:mm' \n",
    "DATE_FORMAT_ACTUAL = 'dd.MM.yyyy HH:mm:ss' # both formats are used\n",
    "\n",
    "df_processed = df.withColumn('ArrivalTimeScheduledDate', to_timestamp(df.ArrivalTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "df_processed = df_processed.withColumn('DepartureTimeScheduledDate', to_timestamp(df_processed.DepartureTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "\n",
    "df_processed = df_processed.withColumn('ArrivalTimeScheduled', unix_timestamp(df_processed.ArrivalTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "df_processed = df_processed.withColumn('ArrivalTimeActual', unix_timestamp(df_processed.ArrivalTimeActual, DATE_FORMAT_ACTUAL))\n",
    "df_processed = df_processed.withColumn('DepartureTimeScheduled', unix_timestamp(df_processed.DepartureTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "df_processed = df_processed.withColumn('DepartureTimeActual', unix_timestamp(df_processed.DepartureTimeActual, DATE_FORMAT_ACTUAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into how the data looks so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(TripDate='13.09.2017', TripId='80:06____:17010:000', OperatorId='80:06____', OperatorAbbrv='DB', OperatorName='DB Regio AG', ProductId='Zug', LineId='17010', LineType='RE', UmlaufId=None, TransportType='RE', AdditionalTrip='false', FailedTrip='false', BPUIC='8500090', StopName='Basel Bad Bf', ArrivalTimeScheduled=None, ArrivalTimeActual=None, ArrivalTimeActualStatus='PROGNOSE', DepartureTimeScheduled=1505274300, DepartureTimeActual=1505274300, DepartureTimeActualStatus='PROGNOSE', SkipStation='false', ArrivalTimeScheduledDate=None, DepartureTimeScheduledDate=datetime.datetime(2017, 9, 13, 5, 45))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also add the hour of departure and of the arrival to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[line_id: string, product_id: string, stop_name: string, additional_trip: string, arrival_hour: string, departure_hour: string, day_of_week: string, delta_arrival: float, delta_departure: float]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType, StringType\n",
    "from pyspark.sql.functions import hour, to_date, date_format, month\n",
    "\n",
    "df_to_classify = df_processed.select(\n",
    "    df_processed.LineId.alias('line_id'), \n",
    "    df_processed.ProductId.alias('product_id'), \n",
    "    df_processed.StopName.alias('stop_name'),\n",
    "    df_processed.AdditionalTrip.alias('additional_trip'), \n",
    "    hour(df_processed.ArrivalTimeScheduledDate).alias(\"arrival_hour\").astype(StringType()),\n",
    "    hour(df_processed.DepartureTimeScheduledDate).alias(\"departure_hour\").astype(StringType()),\n",
    "    date_format(to_date(df_processed.TripDate, 'dd.MM.yyyy'), 'u').alias(\"day_of_week\"),\n",
    "    ((df_processed.ArrivalTimeActual - df_processed.ArrivalTimeScheduled)).alias(\"delta_arrival\").astype(FloatType()),\n",
    "    ((df_processed.DepartureTimeActual - df_processed.DepartureTimeScheduled)).alias(\"delta_departure\").astype(FloatType()))\n",
    "\n",
    "df_to_classify.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(line_id='17010', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='5', day_of_week='3', delta_arrival=None, delta_departure=0.0),\n",
       " Row(line_id='17012', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='6', day_of_week='3', delta_arrival=None, delta_departure=0.0),\n",
       " Row(line_id='17013', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour='6', departure_hour=None, day_of_week='3', delta_arrival=180.0, delta_departure=None),\n",
       " Row(line_id='17014', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='9', day_of_week='3', delta_arrival=None, delta_departure=0.0),\n",
       " Row(line_id='17015', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour='8', departure_hour=None, day_of_week='3', delta_arrival=300.0, delta_departure=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_classify.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for using the Decision Tree Regressor, and because each feature is in fact categorial, we must each one of them using a *StringIndexer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def transform_dataset(dataset, departure):\n",
    "    '''\n",
    "    Function that transforms a dataset, adding for each categorial feature a column, which represents the output of the \n",
    "    StringIndexer applied to that column. \n",
    "    \n",
    "    Parameters:\n",
    "        - dataset: the dataset to be processed\n",
    "        - departure: True if the dataset is for departures, False otherwise\n",
    "    '''\n",
    "    \n",
    "    line_id_indexer = StringIndexer(inputCol=\"line_id\", outputCol=\"line_id_cat\", handleInvalid='keep') # keep nulls \n",
    "    product_id_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_id_cat\", handleInvalid='skip')\n",
    "    stop_name_indexer = StringIndexer(inputCol=\"stop_name\", outputCol=\"stop_name_cat\", handleInvalid='skip')\n",
    "    additional_trip_indexer = StringIndexer(inputCol=\"additional_trip\", outputCol=\"additional_trip_cat\", handleInvalid='skip')\n",
    "    day_of_week_indexer = StringIndexer(inputCol=\"day_of_week\", outputCol=\"day_of_week_cat\", handleInvalid='skip')\n",
    "    departure_hour_indexer = StringIndexer(inputCol=\"departure_hour\", outputCol=\"departure_hour_cat\", handleInvalid='skip')\n",
    "    arrival_hour_indexer = StringIndexer(inputCol=\"arrival_hour\", outputCol=\"arrival_hour_cat\", handleInvalid='skip')\n",
    "\n",
    "    indexers = [line_id_indexer, product_id_indexer, stop_name_indexer, additional_trip_indexer,day_of_week_indexer]\n",
    "    \n",
    "    if departure:\n",
    "        indexers.append(departure_hour_indexer)\n",
    "    else:\n",
    "        indexers.append(arrival_hour_indexer)\n",
    "\n",
    "    indexed = dataset\n",
    "\n",
    "    for indexer in indexers:\n",
    "        indexed = indexer.fit(indexed).transform(indexed) # add columns to dataset\n",
    "        \n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the *VectorAssembler* to construct the column for features, which will be used by the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "def compute_features_column(dataset, is_departure):\n",
    "    '''\n",
    "    Function that computes the features column for the given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        - dataset: the dataset to compute the features column for\n",
    "        - is_departure: True is dataset is used for departures, False otherwise.\n",
    "    '''\n",
    "    input_cols = ['line_id_cat', 'product_id_cat', 'stop_name_cat', 'additional_trip_cat', 'day_of_week_cat']\n",
    "    \n",
    "    if is_departure:\n",
    "        input_cols.append('departure_hour_cat') # departure dataset\n",
    "    else:\n",
    "        input_cols.append('arrival_hour_cat') # arrival dataset\n",
    "        \n",
    "    vector_assembler = VectorAssembler(inputCols = input_cols, outputCol = 'features')\n",
    "    dataset = transform_dataset(dataset, is_departure) # add categorial features\n",
    "    \n",
    "    df_features = vector_assembler.transform(dataset) # add features column\n",
    "    # Use VectorIndexer to make sure that the added features are recognized as categorical\n",
    "    \n",
    "    featureIndexer = \\\n",
    "        VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=100000000).fit(df_features)\n",
    "    \n",
    "    df_features = featureIndexer.transform(df_features) # transform features to categorical\n",
    "    \n",
    "    if is_departure:\n",
    "        df_final = df_features.select(df_features.indexedFeatures, df_features.delta_departure.alias(\"delta\"))\n",
    "    else:\n",
    "        df_final = df_features.select(df_features.indexedFeatures, df_features.delta_arrival.alias(\"delta\"))\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct our datasets to input to the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct departures dataset\n",
    "df_departure_to_regress = df_to_classify.filter(\n",
    "    df_to_classify.departure_hour.isNotNull() & # filter only departures\n",
    "    df_to_classify.delta_departure.isNotNull())\n",
    "\n",
    "df_departure = compute_features_column(df_departure_to_regress, is_departure=True)\n",
    "\n",
    "# Construct arrivals dataset\n",
    "df_arrival_to_regress = df_to_classify.filter(\n",
    "    df_to_classify.arrival_hour.isNotNull() & # filter only arrivals\n",
    "    df_to_classify.delta_arrival.isNotNull())\n",
    "\n",
    "df_arrival = compute_features_column(df_arrival_to_regress, is_departure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the generated dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(indexedFeatures=DenseVector([14483.0, 2.0, 2333.0, 0.0, 2.0, 18.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([10292.0, 2.0, 2333.0, 0.0, 2.0, 11.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([10275.0, 2.0, 2333.0, 0.0, 2.0, 13.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([13971.0, 2.0, 2333.0, 0.0, 2.0, 12.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([14059.0, 2.0, 2333.0, 0.0, 2.0, 8.0]), delta=780.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_departure.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(indexedFeatures=DenseVector([10223.0, 2.0, 2006.0, 0.0, 2.0, 11.0]), delta=180.0),\n",
       " Row(indexedFeatures=DenseVector([9633.0, 2.0, 2006.0, 0.0, 2.0, 4.0]), delta=300.0),\n",
       " Row(indexedFeatures=DenseVector([12909.0, 2.0, 2006.0, 0.0, 2.0, 12.0]), delta=60.0),\n",
       " Row(indexedFeatures=DenseVector([12924.0, 2.0, 2006.0, 0.0, 2.0, 8.0]), delta=300.0),\n",
       " Row(indexedFeatures=DenseVector([10172.0, 2.0, 2006.0, 0.0, 2.0, 6.0]), delta=300.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arrival.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the function for training the Decision Tree Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "def train_regressor(dataset):\n",
    "    dt = DecisionTreeRegressor(featuresCol ='indexedFeatures', labelCol = 'delta', maxBins=100000000, maxDepth=3)\n",
    "    dt_model = dt.fit(dataset)\n",
    "    \n",
    "    return dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the decision trees for both datasets and we extract the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances departures: (6,[0,2,5],[0.284510290083,0.0974031881251,0.618086521792])\n",
      "Feature importances arrivals: (6,[0,2,3,5],[0.334593468,0.0606024087098,0.0354578885646,0.569346234726])\n"
     ]
    }
   ],
   "source": [
    "# Get most important fetrain_regressorpartures dataset\n",
    "regressor_departures = train_regressor(df_departure)\n",
    "print(\"Feature importances departures: {}\".format(regressor_departures.featureImportances))\n",
    "\n",
    "# Get most important features for departures dataset\n",
    "regressor_arrivals = train_regressor(df_arrival)\n",
    "print(\"Feature importances arrivals: {}\".format(regressor_arrivals.featureImportances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the 3 most important features are, in both cases, the *hour*, the *line_id* and the *stop_name*. We can see that everything makes very much sense, because we have big differences of delays between normal hours and rush hours, for example, and also specific stops and routes have usually more delays than the others.\n",
    "\n",
    "Therefore, we continue by constructing the probability distributions for each possible value of the three most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the probability distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we only consider the three most important features in the two initial datasets. We will consider the unity of time to be the minute from now on, instead of seconds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_best_feat_departures = df_departure_to_regress.select(\n",
    "                df_departure_to_regress.departure_hour,\n",
    "                df_departure_to_regress.stop_name,\n",
    "                df_departure_to_regress.line_id,\n",
    "                (df_departure_to_regress.delta_departure / 60).astype(IntegerType()).alias(\"delta_minutes\"))\n",
    "\n",
    "df_best_feat_departures = df_best_feat_departures.filter(df_best_feat_departures.delta_minutes <= 0) \n",
    "# only keep departures which left on time or earlier, we do not want to base our recommendation on assumption\n",
    "# that a train or bus leaves with a delay.\n",
    "\n",
    "df_best_feat_arrival = df_arrival_to_regress.select(\n",
    "                df_arrival_to_regress.arrival_hour,\n",
    "                df_arrival_to_regress.stop_name,\n",
    "                df_arrival_to_regress.line_id,\n",
    "                (df_arrival_to_regress.delta_arrival / 60).astype(IntegerType()).alias(\"delta_minutes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(departure_hour='5', stop_name='Basel Bad Bf', line_id='17010', delta_minutes=0),\n",
       " Row(departure_hour='6', stop_name='Basel Bad Bf', line_id='17012', delta_minutes=0),\n",
       " Row(departure_hour='9', stop_name='Basel Bad Bf', line_id='17014', delta_minutes=0),\n",
       " Row(departure_hour='10', stop_name='Basel Bad Bf', line_id='17016', delta_minutes=0),\n",
       " Row(departure_hour='14', stop_name='Basel Bad Bf', line_id='17024', delta_minutes=0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best_feat_departures.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to make the distribution of delays for each possible value of the features, for both departures and arrivals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, count, lit\n",
    "\n",
    "df_departures_grouped_count = df_best_feat_departures.groupby( \n",
    "                df_best_feat_departures.departure_hour,\n",
    "                df_best_feat_departures.stop_name,\n",
    "                df_best_feat_departures.line_id,\n",
    "                df_best_feat_departures.delta_minutes).agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "        \n",
    "df_departures_distribution = df_departures_grouped_count.\\\n",
    "                                    groupby('departure_hour', 'stop_name', 'line_id').\\\n",
    "                                    agg(collect_list(struct('delta_minutes', 'count_min')).alias('counts'))\n",
    "\n",
    "# for each value of (departure_hour, stop_name, line_id), we have a list of the form [(delay_minutes, count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(departure_hour='19', stop_name='Basel Bad Bf', line_id='17385', delta_minutes=0, count_min=4),\n",
       " Row(departure_hour='19', stop_name='Ependes', line_id='12172', delta_minutes=0, count_min=118),\n",
       " Row(departure_hour='16', stop_name='Vevey', line_id='12257', delta_minutes=0, count_min=148)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_departures_grouped_count.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------+--------+\n",
      "|departure_hour|  stop_name|line_id|  counts|\n",
      "+--------------+-----------+-------+--------+\n",
      "|             0|        Bex|   3591|[[0,50]]|\n",
      "|             0|Biel/Bienne|   7845|[[0,65]]|\n",
      "+--------------+-----------+-------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_departures_distribution.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_key_for_feature_values(hour, line_id, stop_name):\n",
    "    return '{}#{}#{}'.format(hour, line_id, stop_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collected = df_departures_distribution.collect()\n",
    "\n",
    "distribution_departures = {\n",
    "    compute_key_for_feature_values(x.departure_hour, x.line_id, x.stop_name) : \n",
    "    list(sorted(x.counts, key=lambda y: y[0])) for x in collected}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same now for the arrivals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_arrivals_grouped_count = df_best_feat_arrival.groupby( \n",
    "                df_best_feat_arrival.arrival_hour,\n",
    "                df_best_feat_arrival.stop_name,\n",
    "                df_best_feat_arrival.line_id,\n",
    "                df_best_feat_arrival.delta_minutes).agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "        \n",
    "df_arrivals_distribution = df_arrivals_grouped_count.\\\n",
    "                                    groupby('arrival_hour', 'stop_name', 'line_id').\\\n",
    "                                    agg(collect_list(struct('delta_minutes', 'count_min')).alias('counts'))\n",
    "        \n",
    "collected = df_arrivals_distribution.collect()\n",
    "\n",
    "distribution_arrivals = {\n",
    "    compute_key_for_feature_values(x.arrival_hour, x.line_id, x.stop_name) : \n",
    "    list(sorted(x.counts, key=lambda y: y[0])) for x in collected}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to include a default distribution, for the case we have new data, which was not encountered anymore. We will compute it as the distribution of all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_default_distrib_departures = df_best_feat_departures.groupby('delta_minutes').agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "collected_default = df_default_distrib_departures.collect()\n",
    "default_departures = list(sorted(collected_default, key=lambda x: x[0]))\n",
    "\n",
    "df_default_distrib_arrivals = df_best_feat_arrival.groupby('delta_minutes').agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "collected_default = df_default_distrib_arrivals.collect()\n",
    "default_arrivals = list(sorted(collected_default, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add the default values to the dictionary of distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution_departures['default'] = default_departures\n",
    "distribution_arrivals['default'] = default_arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the counts to probabilities, to be able to compute the final quality faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_proba(counts_list):\n",
    "    total_sum = 0\n",
    "    final_proba = []\n",
    "    \n",
    "    for row in counts_list:\n",
    "        total_sum += row.count_min\n",
    "        \n",
    "    for row in counts_list:\n",
    "        final_proba.append((row.delta_minutes, row.count_min / total_sum))\n",
    "        \n",
    "    return final_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution_departures = {k : transform_to_proba(v) for k, v in distribution_departures.items()}\n",
    "distribution_arrivals = {k : transform_to_proba(v) for k, v in distribution_arrivals.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally write the computed dictionaries to file, to be able to load them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "FILE_DISTRIBUTION_DEPARTURES = 'distrib_departures.pic'\n",
    "FILE_DISTRIBUTION_ARRIVALS = 'distrib_arrivals.pic'\n",
    "\n",
    "pickle.dump(distribution_departures, open(FILE_DISTRIBUTION_DEPARTURES, 'wb'))\n",
    "pickle.dump(distribution_arrivals, open(FILE_DISTRIBUTION_ARRIVALS, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exposed API for computing distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last part is to write a function which receives the features of a specific transfer, and it returns the quality of the transfer, by performing the convolution of the corresponding distributions, using the formula:\n",
    "\n",
    "$\\sum\\limits_{t_a }\\Pr[\\mathcal{D}_a = t_a] \\cdot \\Pr[\\mathcal{D}_d = k-1+t_a]$,\n",
    "      \n",
    "where we have taken into consideration the minute needed by the traveler for changing the transport. \n",
    "       \n",
    "Here, we considered $\\mathcal{D}_a$ to be the distribution of arrivals and $\\mathcal{D}_d$ the distribution of departures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "DATE_FORMAT = '%b %d %Y %H:%M:%S'\n",
    "    \n",
    "def compute_quality(arrival_timestamp, departure_timestamp, stop_name, line_id_arr, line_id_dep):\n",
    "    # Dec 31 2017 20:40:49,01\n",
    "    global distribution_departures, distribution_arrivals\n",
    "    \n",
    "    arrival_time = datetime.strptime(arrival_timestamp[:-3], DATE_FORMAT)\n",
    "    departure_time = datetime.strptime(departure_timestamp[:-3], DATE_FORMAT)\n",
    "    \n",
    "    arrival_hour = arrival_time.hour\n",
    "    departure_hour = departure_time.hour\n",
    "    delta_minutes = int((time.mktime(departure_time.timetuple()) - time.mktime(arrival_time.timetuple())) / 60)\n",
    "    \n",
    "    if delta_minutes < 0:\n",
    "        return 0 # impossible to complete the transfer\n",
    "    \n",
    "    departure_key = compute_key_for_feature_values(departure_hour, line_id_dep, stop_name)\n",
    "    if departure_key in distribution_departures:\n",
    "        departure_dist = distribution_departures[departure_key]\n",
    "    else: \n",
    "        departure_dist = distribution_departures['default'] # default distribution\n",
    "       \n",
    "    arrival_key = compute_key_for_feature_values(arrival_hour, line_id_arr, stop_name)\n",
    "    if arrival_key in distribution_arrivals:\n",
    "        arrival_dist = distribution_arrivals[arrival_key]\n",
    "    else: \n",
    "        arrival_dist = distribution_arrivals['default'] # default distribution\n",
    "        \n",
    "    total_proba = 0\n",
    "    \n",
    "    for dep_delay, dep_proba in departure_dist:\n",
    "        for arr_delay, arr_proba in arrival_dist:\n",
    "            \n",
    "            delta_minutes = ((departure_time - arrival_time).seconds // 60) % 60\n",
    "            if delta_minutes >= dep_delay + arr_delay + 1:\n",
    "                total_proba += (dep_proba * arr_proba)\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "    return total_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "FILE_DISTRIBUTION_DEPARTURES = 'distrib_departures.pic'\n",
    "FILE_DISTRIBUTION_ARRIVALS = 'distrib_arrivals.pic'\n",
    "distribution_departures = pickle.load(open(FILE_DISTRIBUTION_DEPARTURES, 'rb'))\n",
    "distribution_arrivals = pickle.load(open(FILE_DISTRIBUTION_ARRIVALS, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8011350894443582\n"
     ]
    }
   ],
   "source": [
    "print(compute_quality('Dec 31 2017 00:40:49,01', 'Dec 31 2017 00:41:58,01', 'Dietikon, Birmensdorferstrasse', '85:849:303','85:849:303'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import pyspark.sql.functions as fct\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from geopy.distance import distance as geo_dist\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets\n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4065\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>final_proj-cdmvansc</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=final_proj-cdmvansc>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('final_proj-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '4g')\n",
    "conf.set('spark.executor.instances', '10')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/datasets/project/istdaten/*/*/*', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = 'TripDate string, TripId string, OperatorId string, OperatorAbbrv string, OperatorName string, ProductId string, LineId string, LineType string, UmlaufId string, TransportType string, AdditionalTrip boolean, FailedTrip boolean, BPUIC string, StopName string, ArrivalTimeScheduled string, ArrivalTimeActual string, ArrivalTimeActualStatus string,     DepartureTimeScheduled string, DepartureTimeActual string, DepartureTimeActualStatus string, SkipStation boolean'\n",
    "columns = list(map(lambda x: x.split()[0],columns.split(',')))\n",
    "\n",
    "for old, new in zip(df.columns, columns):\n",
    "    #print(old, new)\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TripDate: string (nullable = true)\n",
      " |-- TripId: string (nullable = true)\n",
      " |-- OperatorId: string (nullable = true)\n",
      " |-- OperatorAbbrv: string (nullable = true)\n",
      " |-- OperatorName: string (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- LineId: string (nullable = true)\n",
      " |-- LineType: string (nullable = true)\n",
      " |-- UmlaufId: string (nullable = true)\n",
      " |-- TransportType: string (nullable = true)\n",
      " |-- AdditionalTrip: string (nullable = true)\n",
      " |-- FailedTrip: string (nullable = true)\n",
      " |-- BPUIC: string (nullable = true)\n",
      " |-- StopName: string (nullable = true)\n",
      " |-- ArrivalTimeScheduled: string (nullable = true)\n",
      " |-- ArrivalTimeActual: string (nullable = true)\n",
      " |-- ArrivalTimeActualStatus: string (nullable = true)\n",
      " |-- DepartureTimeScheduled: string (nullable = true)\n",
      " |-- DepartureTimeActual: string (nullable = true)\n",
      " |-- DepartureTimeActualStatus: string (nullable = true)\n",
      " |-- SkipStation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp = df.select(fct.split(df.TripId, ':')[2].alias('Line_ID'),\n",
    "              fct.split(df.TripId, ':')[3].alias('Line_ID_spec'),\n",
    "              'LineType', 'ProductId','LineType', 'TripDate', \n",
    "                   'ArrivalTimeScheduled','DepartureTimeScheduled', 'StopName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by read metadata in order to select stop station within 10 km from Zürich "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = spark.read.csv('/datasets/project/metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we can see that we have some duplicated stop name \n",
    "#df_meta.filter(df_meta._c0.contains('Zürich')).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = df_meta.filter(df_meta['_c0'].rlike(\"Lausanne\")).collect()\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta.select(fct.split(df_meta['_c0'], '  ')[1].alias('Long'), \n",
    "                         fct.split(fct.split(df_meta['_c0'], '  ')[2], ' ')[0].alias('Lat'), \n",
    "                         fct.split(df_meta['_c0'], '% ')[1].alias('StopName_Meta') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+\n",
      "|     Long|      Lat|      StopName_Meta|\n",
      "+---------+---------+-------------------+\n",
      "|26.074412|44.446770|          Bucuresti|\n",
      "| 1.811446|50.901549|             Calais|\n",
      "| 1.075329|51.284212|         Canterbury|\n",
      "|-3.543547|50.729172|             Exeter|\n",
      "| 9.733756|46.922368|            Fideris|\n",
      "| 8.571251|50.051219|Frankfurt Flughafen|\n",
      "|18.643803|54.355520|             Gdansk|\n",
      "| 7.389462|47.191804|           Grenchen|\n",
      "|29.019602|40.996348|           Istanbul|\n",
      "| 9.873959|48.577852|  Amstetten (Württ)|\n",
      "| 4.786044|43.921937|            Avignon|\n",
      "| 2.140369|41.378914|          Barcelona|\n",
      "| 7.589551|47.547405|              Basel|\n",
      "| 7.395229|46.937482|       Bern Bümpliz|\n",
      "|-1.899480|52.483627|         Birmingham|\n",
      "| 6.838953|46.949588|          Boudry TN|\n",
      "|17.106466|48.158910|         Bratislava|\n",
      "| 4.335694|50.835376|          Bruxelles|\n",
      "|-2.979650|53.404289|          Liverpool|\n",
      "| 8.500049|47.114619|         Lothenbach|\n",
      "+---------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "print(len(df_meta.filter(df_meta['StopName_Meta'].rlike(\"Lausanne\")).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Again we can see that we have many occurance of Zurich with different coordinate\n",
    "#df_meta.filter(df_meta.StopName_Meta == 'Zürich').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta.withColumn(\"Long\", df_meta[\"Long\"].cast(FloatType()))\n",
    "df_meta = df_meta.withColumn(\"Lat\", df_meta[\"Lat\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Long: float (nullable = true)\n",
      " |-- Lat: float (nullable = true)\n",
      " |-- StopName_Meta: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is many duplicate name with different coordinate. \n",
    "For example we find many time Lausanne, after investigatin we understand that all the subway station where simply Lausanne. We decide to fill that problem using another dataset in order to merge them. \n",
    "\n",
    "We decide to merge the two dataset using coordinate, in order to do this we round coordinate to match them. A round at 3 decimal change the precission by max 135m. For example Google Maps use 6 decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we only keep point in/near switzerland we decide to do this by draw a square arount the country and keep point inside.  Here we find the extreme points of switzerland: \n",
    "https://fr.wikipedia.org/wiki/Liste_de_points_extr%C3%AAmes_de_la_Suisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25935"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta.filter(df_meta.Lat.between(45.490404, 47.485074))\n",
    "df_meta = df_meta.filter(df_meta.Long.between(5.572263, 10.2931))                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22723"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we see the minimum precision we have in our dataset in order to round all coordinate to this precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slen = udf(lambda s: len(str(s).split('.')[1]), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(min(lat_len)=6)]\n",
      "[Row(min(lon_len)=6)]\n"
     ]
    }
   ],
   "source": [
    "df_meta = df_meta.withColumn(\"lat_len\", slen(df_meta.Lat))\n",
    "df_meta = df_meta.withColumn(\"lon_len\", slen(df_meta.Long))\n",
    "#df_meta = df_meta.withColumn(\"precision\", min(df_meta.lat_len, df_meta.lon_len))\n",
    "print(df_meta.agg({\"lat_len\": \"min\"}).collect())\n",
    "print(df_meta.agg({\"lon_len\": \"min\"}).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a precision of 6 digit which is sufficient for our work. See why df_meta.show(5) not always display the same number of digit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+-------+-------+\n",
      "|    Long|      Lat|StopName_Meta|lat_len|lon_len|\n",
      "+--------+---------+-------------+-------+-------+\n",
      "|9.733756|46.922367|      Fideris|     15|     15|\n",
      "|7.389462|47.191803|     Grenchen|     15|     15|\n",
      "|7.395229| 46.93748| Bern Bümpliz|     14|     15|\n",
      "|6.838953| 46.94959|    Boudry TN|     15|     15|\n",
      "|8.500049| 47.11462|   Lothenbach|     15|     15|\n",
      "+--------+---------+-------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta.select('Long', 'Lat', 'StopName_Meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round_6 = udf(lambda s: round(s, 6), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta.withColumn(\"Round_Long\", round_6(df_meta.Long))\n",
    "df_meta = df_meta.withColumn(\"Round_Lat\", round_6(df_meta.Lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+----------+---------+\n",
      "|    Long|      Lat|StopName_Meta|Round_Long|Round_Lat|\n",
      "+--------+---------+-------------+----------+---------+\n",
      "|9.733756|46.922367|      Fideris|  9.733756|46.922367|\n",
      "|7.389462|47.191803|     Grenchen|  7.389462|47.191803|\n",
      "|7.395229| 46.93748| Bern Bümpliz|  7.395229|46.937481|\n",
      "|6.838953| 46.94959|    Boudry TN|  6.838953|46.949589|\n",
      "|8.500049| 47.11462|   Lothenbach|  8.500049| 47.11462|\n",
      "+--------+---------+-------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22696\n",
      "22671\n"
     ]
    }
   ],
   "source": [
    "print(df_meta.distinct().count())\n",
    "print(df_meta.select('Round_Lat', 'Round_Long').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use another dataset to fil missing names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat_stop</th>\n",
       "      <th>Long_stop</th>\n",
       "      <th>StopName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45.989901</td>\n",
       "      <td>8.345062</td>\n",
       "      <td>Anzola, chiesa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.167251</td>\n",
       "      <td>8.345807</td>\n",
       "      <td>Altoggio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.060122</td>\n",
       "      <td>8.113620</td>\n",
       "      <td>Antronapiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.989870</td>\n",
       "      <td>8.345717</td>\n",
       "      <td>Anzola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46.261498</td>\n",
       "      <td>8.319253</td>\n",
       "      <td>Baceno</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lat_stop  Long_stop        StopName\n",
       "0  45.989901   8.345062  Anzola, chiesa\n",
       "1  46.167251   8.345807        Altoggio\n",
       "2  46.060122   8.113620    Antronapiana\n",
       "3  45.989870   8.345717          Anzola\n",
       "4  46.261498   8.319253          Baceno"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stops.txt', 'r') as file: \n",
    "    one_splitted = file.readline().strip().split(\",\")\n",
    "    file_lines = [line.strip().split('\"') for line in file.readlines()]\n",
    "    \n",
    "stop_names = [x[3] for x in file_lines]\n",
    "Lat = [float(x[5]) for x in file_lines]\n",
    "Long = [float(x[7]) for x in file_lines]\n",
    "\n",
    "df_stop = pd.DataFrame({\n",
    "        \"StopName\": stop_names, \n",
    "        \"Lat_stop\": Lat, \n",
    "        \"Long_stop\": Long,   \n",
    "    })\n",
    "df_stop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|        Lat_stop|       Long_stop|            StopName|\n",
      "+----------------+----------------+--------------------+\n",
      "|45.9899010293845|8.34506152974108|      Anzola, chiesa|\n",
      "|46.1672513851495|  8.345807131427|            Altoggio|\n",
      "| 46.060121674738|8.11361957990831|        Antronapiana|\n",
      "|45.9898698225697|8.34571729989858|              Anzola|\n",
      "|46.2614983591677|8.31925293162473|              Baceno|\n",
      "|46.0790618438814|8.29927439970313|Beura Cardezza, c...|\n",
      "|46.1222963432243|8.21077237789936|Bognanco, T. Vill...|\n",
      "|46.0656504576122|8.26113193273411|           Boschetto|\n",
      "|46.2978807772998| 8.3626325767009|            Cadarese|\n",
      "|46.1340194356792|8.28619492916453|               Caddo|\n",
      "|46.0916476333918|8.28041876188684|              Calice|\n",
      "|45.9695691829797|8.04585965801774|            Campioli|\n",
      "|46.4091810825782| 8.4117524564434|    Cascate del Toce|\n",
      "|46.0205875326422| 8.2148866619012|         Castiglione|\n",
      "|45.9710364221151|8.06992552448265|       Ceppo Morelli|\n",
      "|46.3530849443472|8.42787721579558|Chiesa (Val Forma...|\n",
      "|46.0967496675661|8.31182386422403| Cosasca di Trontano|\n",
      "|46.0664046229574| 8.2328978833503|              Cresti|\n",
      "|46.1562758593614|8.30343359946918|      Crevoladossola|\n",
      "|46.2141837457637|8.32131905677849|        Crodo, Bagni|\n",
      "+----------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mySchema = StructType([ StructField(\"Lat_stop\", DoubleType(), True)\\\n",
    "                        ,StructField(\"Long_stop\", DoubleType(), True)\\\n",
    "                        ,StructField(\"StopName\", StringType(), True) ])\n",
    "df_stop = spark.createDataFrame(df_stop, mySchema)\n",
    "df_stop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(min(lat_len)=1)]\n",
      "[Row(min(lon_len)=1)]\n",
      "+----------------+------------+--------------------+-------+-------+\n",
      "|        Lat_stop|   Long_stop|            StopName|lat_len|lon_len|\n",
      "+----------------+------------+--------------------+-------+-------+\n",
      "|             0.0|         0.0|     Isola Superiore|      1|      1|\n",
      "|47.3611471419894|7.3110197892|Develier, St-Chri...|     13|     10|\n",
      "+----------------+------------+--------------------+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stop = df_stop.withColumn(\"lat_len\", slen(df_stop.Lat_stop))\n",
    "df_stop = df_stop.withColumn(\"lon_len\", slen(df_stop.Long_stop))\n",
    "#df_meta = df_meta.withColumn(\"precision\", min(df_meta.lat_len, df_meta.lon_len))\n",
    "print(df_stop.agg({\"lat_len\": \"min\"}).collect())\n",
    "print(df_stop.agg({\"lon_len\": \"min\"}).collect())\n",
    "\n",
    "df_stop.orderBy('lon_len').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Lat_stop=0.0, Long_stop=0.0, StopName='Isola Superiore', lat_len=1, lon_len=1)]\n"
     ]
    }
   ],
   "source": [
    "print(df_stop.filter(df_stop['StopName'].rlike(\"Isola Superiore\")).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that this can from an error in the dataset, we use google maps to find the good coordinate of Isola Superiore which is: Isola Superiore: 45.901230 - 8.520450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stop = df_stop.withColumn(\"Lat_stop\", \\\n",
    "              when(df_stop[\"StopName\"] == 'Isola Superiore', 45.901230).otherwise(df_stop[\"Lat_stop\"]))\n",
    "df_stop = df_stop.withColumn(\"Long_stop\", \\\n",
    "              when(df_stop[\"StopName\"] == 'Isola Superiore', 8.520450).otherwise(df_stop[\"Long_stop\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we again round all the coordinate by 6 in order to merge both of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stop = df_stop.withColumn(\"Round_Long\", round_6(df_stop.Long_stop))\n",
    "df_stop = df_stop.withColumn(\"Round_Lat\", round_6(df_stop.Lat_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Df_meta = df_meta.join(df_stop, on = ['Round_Lat', 'Round_Long'], how='outer') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "print(df_meta.filter(df_meta['StopName_Meta'].like(\"Lausanne\")).count())\n",
    "print(Df_meta.filter(Df_meta['StopName_Meta'].like(\"Lausanne\") & Df_meta['StopName'].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for the example of Lausanne we just recover 2 name over about a hundred. \n",
    "After investigation we find the coordidate for particular station in both dataset: \n",
    "<br/>\n",
    "<br/>Lausanne Malley: 46.524212 - 6.603306 -- 46.524211 - 6.603309\n",
    "<br/>Lausanne Bourdonette: 46.523466 - 6.589805 -- 46.523465 - 6.589807\n",
    "<br/>Lausanne Provence: 46.523384 - 6.608102 -- 46.523382 - 6.608106\n",
    "\n",
    "We can see that each time our merge fail for 1 digit\n",
    "\n",
    "We try again with a round at 5 digits whith is still a very good precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "round_5 = udf(lambda s: round(s, 5), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_meta = df_meta.withColumn(\"Round_Long\", round_5(df_meta.Long))\n",
    "df_meta = df_meta.withColumn(\"Round_Lat\", round_5(df_meta.Lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_stop = df_stop.withColumn(\"Round_Long\", round_5(df_stop.Long_stop))\n",
    "df_stop = df_stop.withColumn(\"Round_Lat\", round_5(df_stop.Lat_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Df_meta = df_meta.join(df_stop, on = ['Round_Lat', 'Round_Long'], how='outer') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(df_meta.filter(df_meta['StopName_Meta'].like(\"Lausanne\")).count())\n",
    "print(Df_meta.filter(Df_meta['StopName_Meta'].like(\"Lausanne\") & Df_meta['StopName'].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now achieved a satisfactory result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Round_Lat: double (nullable = true)\n",
      " |-- Round_Long: double (nullable = true)\n",
      " |-- Long: float (nullable = true)\n",
      " |-- Lat: float (nullable = true)\n",
      " |-- StopName_Meta: string (nullable = true)\n",
      " |-- Lat_stop: double (nullable = true)\n",
      " |-- Long_stop: double (nullable = true)\n",
      " |-- StopName: string (nullable = true)\n",
      " |-- lat_len: integer (nullable = true)\n",
      " |-- lon_len: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------------+--------------------+\n",
      "|    Long|      Lat|   StopName_Meta|            StopName|\n",
      "+--------+---------+----------------+--------------------+\n",
      "|    null|     null|            null|Macugnaga, Pestarena|\n",
      "|    null|     null|            null| Lugano, Via Ginevra|\n",
      "|    null|     null|            null|      Gandria, Paese|\n",
      "|    null|     null|            null|               Gozzi|\n",
      "|8.943882|46.034714|        Cureglia|   Cureglia, Rotonda|\n",
      "|    null|     null|            null|        Bogno, Paese|\n",
      "|6.090986| 46.15237|           Perly|                null|\n",
      "|6.044045|46.161507|        Laconnex|Laconnex, Chemin ...|\n",
      "|8.912559|46.179436|         Agarone|                null|\n",
      "|8.699336| 46.18245|        Cresmino|      Cresmino, Case|\n",
      "|6.246757|46.183704|       Annemasse|Annemasse, Généra...|\n",
      "|7.393176| 46.19771|Les Mayens-de-S.|Les Mayens-de-S.,...|\n",
      "|6.167676| 46.20001|          Genève|  Genève, Amandolier|\n",
      "|6.157857|46.203766|          Genève|                null|\n",
      "|    null|     null|            null|Sion, Maragnénaz ...|\n",
      "|7.272802|46.223717|          Vétroz|                null|\n",
      "| 7.52549| 46.28712|          Sierre|Sierre, Maison Rouge|\n",
      "|    null|     null|            null| Versoix, Pont-Céard|\n",
      "| 7.52412|46.290703|          Sierre|      Sierre, Liddes|\n",
      "|7.999969|46.310394|            Brig|                null|\n",
      "+--------+---------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Df_meta = Df_meta.select('Long', 'Lat', 'StopName_Meta', 'StopName')\n",
    "Df_meta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Df_meta = Df_meta.na.drop(subset=[\"Long\", 'Lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Df_meta = Df_meta.withColumn(\"StopName_Meta\", \\\n",
    "              when(Df_meta[\"StopName\"].isNotNull(), Df_meta[\"StopName\"]).otherwise(Df_meta[\"StopName_Meta\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Df_meta = Df_meta.select('StopName_Meta', 'Lat', 'Long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lat_long(name): \n",
    "    tmp = Df_meta.select('Lat', 'Long').filter(Df_meta['StopName_Meta'].like(name)).collect()\n",
    "    if(len(tmp) == 0): \n",
    "        assert \"Probleme with the location {}\".format(name)\n",
    "    tmp = tmp[0]\n",
    "    lat = str(tmp).split('=')[1].split(',')[0]\n",
    "    long = str(tmp).split('=')[2].split(')')[0]\n",
    "    return lat, long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_request(fromPlace, toPlace, departure, Months, Days, Hours, AM_PM, Minutes, Seconds, lat_long_from = False, lat_long_to = False):\n",
    "\n",
    "    \n",
    "    if (fromPlace.split(' ')[0] == 'stop'):\n",
    "        fromPlace = fromPlace[5:-1]\n",
    "    if lat_long_from == False:\n",
    "        lat_from, long_from = get_lat_long(fromPlace)\n",
    "    else:\n",
    "        lat_from, long_from = lat_long_from[0], lat_long_from[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if (toPlace.split(' ')[0] == 'stop'):\n",
    "        toPlace = toPlace[5:-1]\n",
    "    #print('TOOOOO PLACE ################## {} {}'.format(toPlace, len(toPlace)))    \n",
    "    if lat_long_to == False:\n",
    "        lat_to, long_to = get_lat_long(toPlace)\n",
    "    else:\n",
    "        lat_to, long_to = lat_long_to[0], lat_long_to[1]\n",
    "    \n",
    "    url = 'http://10.90.38.21:8829/otp/routers/default/plan?fromPlace=stop+'\n",
    "    url += '+'.join(fromPlace.split()) +  '+%3A%3A' + str(lat_from) + '%2C' + str(long_from)\n",
    "    url += '&toPlace=stop+' +  '+'.join(toPlace.split()) +  '+%3A%3A' + str(lat_to) + '%2C' + str(long_to)\n",
    "    url += '&time={}%3A{}{}&date={}-{}-2018&mode=TRANSIT%2CWALK&maxWalkDistance=804.672&arriveBy={}&wheelchair=false&locale=en&numItineraries=3'.format(Hours, Minutes, AM_PM, Months, Days, not(departure))\n",
    "    #url += '&time={}&date={}-{}-2018&mode=TRANSIT%2CWALK&maxWalkDistance=804.672&arriveBy=false&wheelchair=false&locale=en&numItineraries=3&departure=true'.format(Datetime, Months, Days)\n",
    "    #print(url)\n",
    "    r = requests.get(url)\n",
    "    #print(r)\n",
    "    #print(r.json())\n",
    "    #read_json(r.json())\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create itineraries from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json_extract_itineraries(json_data, df_BT):\n",
    "    info_list = []\n",
    "    for route in json_data['plan']['itineraries']:\n",
    "        #Here we show the 3 different path\n",
    "       # print('-------- Route---------\\n')\n",
    "        info_list_route = []\n",
    "        for step in route['legs']: \n",
    "            #Here we show all the step of the route   \n",
    "           # print('---Step---\\n')\n",
    "            #print(step)\n",
    "            mode = step['mode']\n",
    "            from_ = step['from']['name']\n",
    "            lat_from = step['from']['lat']\n",
    "            lon_from = step['from']['lon']\n",
    "            to_ = step['to']['name']\n",
    "            lat_to = step['to']['lat']\n",
    "            lon_to = step['to']['lon']\n",
    "            \n",
    "            start_time = str(step['from']['departure'])\n",
    "            departure_time = time.strftime(\"%b %d %Y %H:%M:%S,%M\", time.localtime(float(start_time[:len(start_time)-3])))\n",
    "            end_time = str(step['endTime'])\n",
    "            arrival_time = time.strftime(\"%b %d %Y %H:%M:%S,%M\", time.localtime(float(end_time[:len(end_time)-3])))\n",
    "            duration = str(step['duration'])\n",
    "            \n",
    "            route_id = 0\n",
    "            trip_id = 0\n",
    "            agency_name = 'unknown'\n",
    "            if ('routeShortName' in step.keys()):\n",
    "                route_id = step['routeShortName']\n",
    "            if('tripShortName' in step.keys()):\n",
    "                trip_id = step['tripShortName']\n",
    "            if('agencyName' in step.keys()):\n",
    "                agency_name = step['agencyName']\n",
    "            line_id = trip_id\n",
    "            if mode != 'RAIL' and mode!='WALK':\n",
    "                if mode == 'BUS' or mode == 'Bus':\n",
    "                    tmp = df_BT.where((col('ProductId') == 'Bus') | (col('ProductId') == 'BUS')).where(col('OperatorName') == agency_name).where(col('LineType')==route_id).head(1)\n",
    "                elif mode =='TRAM' or mode == 'Tram':\n",
    "                    tmp = df_BT.where((col('ProductId') == 'Tram')).where(col('OperatorName') == agency_name).where(col('LineType')==route_id).head(1)\n",
    "                else: \n",
    "                    tmp = df_BT.where((col('ProductId') == mode)).where(col('OperatorName') == agency_name).where(col('LineType')==route_id).head(1)\n",
    "                #print(agency_name)\n",
    "                #print(mode)\n",
    "                #print((df_BT.where(col('OperatorName') == agency_name).where((col('ProductId') == 'BUS') | (col('ProductId') == 'BUS')).head(2)))\n",
    "                if len(tmp) == 0:\n",
    "                    line_id = 'unknown'\n",
    "                else:\n",
    "                    line_id = tmp[0].asDict()['LineId']\n",
    "                #print(final_id)\n",
    "            \n",
    "           # print('The product id is {}:'.format(mode))\n",
    "           # print('Trip from {} at {} to {} at {} with {}'.format(from_,departure_time,to_,arrival_time, mode))\n",
    "           # if('tripShortName' in step.keys()):\n",
    "           #     print('The trip ID is {}'.format(trip_id))\n",
    "           # if('routeShortName' in step.keys()):\n",
    "           #     print('The route ID is {}'.format(route_id))\n",
    "        \n",
    "            info_list_route.append({'product_id': mode, 'from': from_, 'lat_long_from': [lat_from, lon_from] ,'departure_time':departure_time,'to':to_, 'lat_long_to': [lat_to, lon_to], 'arrival_time':arrival_time, 'line_id': line_id})\n",
    "        info_list.append(info_list_route)\n",
    "    return info_list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select itineraries respecting the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comp_itinerary_quality(itinerary):\n",
    "    itinerary_quality_ = 1\n",
    "    for j_ in range(len(itinerary)-1):\n",
    "        leg_1 = itinerary[j_]\n",
    "        leg_2 = itinerary[j_+1]    \n",
    "        \n",
    "        transfer_quality = compute_quality(leg_1['arrival_time'], leg_2['departure_time'], leg_1['to'], leg_1['line_id'], leg_2['line_id'])\n",
    "        #print(transfer_quality)\n",
    "        itinerary_quality_ = itinerary_quality_ * transfer_quality\n",
    "    return itinerary_quality_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_with_quality(itinerary_list, Quality):\n",
    "    itinerary_quality_ = [0,]*len(itinerary_list)\n",
    "    #print('Quality of itineraries:  ')\n",
    "    for i_ in range(len(itinerary_list)):\n",
    "        itinerary_quality_[i_] = comp_itinerary_quality(itinerary_list[i_])\n",
    "        #print('Itinerary number {}, quality: {}'.format(i_,itinerary_quality_[i_]))\n",
    "    itinerary_list_accepted = np.array(itinerary_list)[[it_>Quality for it_ in itinerary_quality_]].tolist()\n",
    "    itinerary_list_refused = np.array(itinerary_list)[[not(it_>Quality) for it_ in itinerary_quality_]].tolist()\n",
    "    return itinerary_list_accepted, itinerary_list_refused"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore itineraries \"around\" a too-low-quality itinerary TODO: make it an actual tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_to_cells(date):\n",
    "    Month_dict= {'Jan':1,'Feb':2,'Mar':3,'Apr':4,'May':5,'Jun':6,'Jul':7,'Aug':8,'Sep':9,'Oct':10,'Nov':11,'Dec':12}\n",
    "    Months = Month_dict[date.split(' ')[0]]\n",
    "    Days = date.split(' ')[1]\n",
    "    Hours = str(int(date.split(' ')[3].split(':')[0])%12)\n",
    "    Minutes = date.split(' ')[3].split(':')[1]\n",
    "    AM_PM = 'AM'\n",
    "    if int(int(date.split(' ')[3].split(':')[0])/12) == 1:\n",
    "        AM_PM = 'PM'\n",
    "    Seconds = date.split(' ')[3].split(':')[2].split(',')[0]\n",
    "    \n",
    "    return Months, Days, Hours, Minutes, AM_PM, Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_itineraries(itinerary, df_BT, quality):\n",
    "    itinerary_list = []\n",
    "    for j_ in range(len(itinerary)-1):\n",
    "        #leg_1 = itinerary[j_]\n",
    "        if comp_itinerary_quality(itinerary[0:j_+1]) < quality:\n",
    "            continue\n",
    "        arr_month, arr_day, arr_hour, arr_minute, arr_AM_PM, arr_second = date_to_cells(itinerary[j_]['arrival_time'])\n",
    "        #new_semi_its = request_with_quality(fromPlace = leg_1['to'], toPlace = itinerary[-1]['to'], Months = arr_month, Days = arr_day, Hours = arr_hour, Minutes = arr_minute, Seconds = arr_second, AM_PM = arr_AM_PM, departure = True, Quality = quality, lat_long_from = leg_1['lat_long_from'], lat_long_to = itinerary[-1]['lat_long_to'] )\n",
    "        temp_json = return_request(fromPlace = itinerary[j_]['to'], toPlace = itinerary[-1]['to'], Months = arr_month, Days = arr_day, Hours = arr_hour, Minutes = arr_minute, Seconds = arr_second, AM_PM = arr_AM_PM, departure = True, lat_long_from = itinerary[j_]['lat_long_to'], lat_long_to = itinerary[-1]['lat_long_to'])\n",
    "        #print(temp_json)\n",
    "        new_partial_its = read_json_extract_itineraries(temp_json, df_BT)\n",
    "        new_itineraries = [np.append(itinerary[:j_+1],new_partial_its[k_]).tolist() for k_ in range(len(new_partial_its))]\n",
    "        \n",
    "        itinerary_list.extend(new_itineraries)\n",
    "    return itinerary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the json of quickest itineraries from local OTP server\n",
    "test_json = return_request(fromPlace=fromPlace_ , toPlace= toPlace_ ,Months = Months_, Days= Days_, Hours= Hours_, AM_PM = AM_PM_, Minutes = Minutes_, Seconds = Seconds_, departure = departure_)\n",
    "#Create Dataframe to find LineId from ProductId, LineType and OperatorName. Relevant for Bus and Tram\n",
    "df_BT = df.where(col('ProductId') != 'Zug').select('ProductId','LineType','OperatorName','LineId').distinct().cache()\n",
    "#Read json and create itinerary list of dicts\n",
    "itinerary_test_list = read_json_extract_itineraries(test_json, df_BT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get news from SBB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_info(date, stopName):\n",
    "    url = 'https://data.sbb.ch/api/records/1.0/search/?dataset=rail-traffic-information&lang=en&rows=1000&sort=validityend&facet=validitybegin&facet=validityend&refine.validitybegin={}'.format(date[0])\n",
    "    tmp = requests.get(url).json()\n",
    "    infos = []\n",
    "    for el in tmp['records']: \n",
    "        end = str(el['fields']['validityend'].split('T')[0]).split('-')\n",
    "        if((int(end[0]) == int(date[0]) and int(end[1]) == int(date[1]) and int(end[2]) < int(date[2])) or (int(end[0]) == int(date[0]) and int(end[1]) < int(date[1])) or (int(end[0]) < int(date[0]))):\n",
    "            break\n",
    "        #print(end)\n",
    "        title = el['fields']['title']\n",
    "        if('End of announcement:' in title): \n",
    "            pass\n",
    "        else:\n",
    "            if(len(title.split(':')) > 1):\n",
    "                title = str(title.split(':')[1])\n",
    "            title = title.replace(' and', '-').replace('engineering work is in progress', '').replace(',','').replace('.', '').replace('Between', '').replace('In', '').replace(' station', '').replace('Work due to a disruption','').strip()\n",
    "            #print(title.split('- '))\n",
    "            for el_title in title.split('- '): \n",
    "                for el_stop in stopName: \n",
    "                    if(el_title.strip() == el_stop.strip()): \n",
    "                        print(el_title)\n",
    "                        infos.append(el['fields']['description'])\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best itineraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_BT = df.where(col('ProductId') != 'Zug').select('ProductId','LineType','OperatorName','LineId').distinct().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itineraries searched: 1\n",
      "Feb 04 2018 18:21:15,21 Feb 04 2018 18:41:00,41\n",
      "itineraries searched: 2\n",
      "Feb 04 2018 18:31:15,31 Feb 04 2018 18:51:00,51\n"
     ]
    }
   ],
   "source": [
    "quality = 0.90\n",
    "fromPlace_ = \"Zürich, Zürichbergstrasse\"\n",
    "toPlace_ = 'Zürich Enge, Bahnhof'\n",
    "Months_ = 2\n",
    "Days_ = 4\n",
    "Hours_ = 6\n",
    "AM_PM_ = 'PM'\n",
    "Minutes_ = 20\n",
    "Seconds_ = 1\n",
    "departure_ = True\n",
    "\n",
    "\n",
    "#Get the json of quickest itineraries from local OTP server\n",
    "test_json = return_request(fromPlace=fromPlace_ , toPlace= toPlace_ ,Months = Months_, Days= Days_, Hours= Hours_, AM_PM = AM_PM_, Minutes = Minutes_, Seconds = Seconds_, departure = departure_)\n",
    "#Create Dataframe to find LineId from ProductId, LineType and OperatorName. Relevant for Bus and Tram\n",
    "#\n",
    "#Read json and create itinerary list of dicts\n",
    "itinerary_first_list = read_json_extract_itineraries(test_json, df_BT)\n",
    "itinerary_acc, itinerary_refu = split_with_quality(itinerary_first_list, Quality = quality)\n",
    "itinerary_searched = []\n",
    "iter_=0\n",
    "\n",
    "## sort bad quality itineraries by arrival time\n",
    "sorter_ids = np.argsort([itinerary_refu[i_][-1]['arrival_time'] for i_ in range(len(itinerary_refu))])\n",
    "itinerary_refu = np.array(itinerary_refu)[sorter_ids].tolist()\n",
    "while len(itinerary_refu) != 0:\n",
    "    if len(itinerary_acc)>=3:\n",
    "        break\n",
    "    iter_+=1\n",
    "    print('itineraries searched: {}'.format(iter_))\n",
    "    itinerary_searched_ = itinerary_refu.pop(0)\n",
    "    itinerary_searched.append(itinerary_searched_)\n",
    "    print(itinerary_searched_[0]['departure_time'], itinerary_searched_[-1]['arrival_time'])\n",
    "    #print(itinerary_searched_)\n",
    "    itinerary_test_list_explored = explore_itineraries(itinerary_searched_, df_BT, quality)\n",
    "    itinerary_acc_explored, itinerary_refu_explored = split_with_quality(itinerary_test_list_explored, quality)\n",
    "    for iti_refu in itinerary_refu_explored:\n",
    "        if not(any([(iti_refu == iti) for iti in itinerary_refu+itinerary_searched])):\n",
    "            itinerary_refu.append(iti_refu)\n",
    "    for iti_acc in itinerary_acc_explored:\n",
    "        if not(any([(iti_acc == iti) for iti in itinerary_acc])):\n",
    "            itinerary_acc.append(iti_acc)\n",
    "    \n",
    "    ## sort by arrival time\n",
    "    sorter_ids = np.argsort([itinerary_refu[i_][-1]['arrival_time'] for i_ in range(len(itinerary_refu))])\n",
    "    itinerary_refu = np.array(itinerary_refu)[sorter_ids].tolist()\n",
    "    \n",
    "\n",
    "## sort selected itineraries by arrival time\n",
    "sorter_ids = np.argsort([itinerary_acc[i_][-1]['arrival_time'] for i_ in range(len(itinerary_acc))])\n",
    "itinerary_acc = np.array(itinerary_acc)[sorter_ids].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itinerary number: 0, quality: 0.016150519696289896, dpt: Feb 04 2018 18:21:15,21, arr: Feb 04 2018 18:41:00,41, transfers: 1\n",
      "Itinerary number: 1, quality: 0.016150519696289896, dpt: Feb 04 2018 18:31:15,31, arr: Feb 04 2018 18:51:00,51, transfers: 1\n",
      "Itinerary number: 2, quality: 0.016150519696289896, dpt: Feb 04 2018 18:41:15,41, arr: Feb 04 2018 19:01:00,01, transfers: 1\n"
     ]
    }
   ],
   "source": [
    "## Compare to initial output from OTP\n",
    "## Print out the arrival time and quality of the three select paths\n",
    "itinerary_initial_quality_ = [0,]*len(itinerary_first_list)\n",
    "for i_ in range(len(itinerary_first_list)):\n",
    "    itinerary_initial_quality_[i_] = comp_itinerary_quality(itinerary_first_list[i_])\n",
    "    print('Fastest itineraries without quality constraint:')\n",
    "    print('Itinerary number: {}, quality: {}, dpt: {}, arr: {}, transfers: {}'.format(i_,itinerary_initial_quality_[i_], itinerary_first_list[i_][0]['departure_time'], itinerary_first_list[i_][-1]['arrival_time'],len(itinerary_first_list[i_])-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itinerary number: 0, quality: 0.9932763142975513, dpt: Feb 04 2018 18:21:15,21, arr: Feb 04 2018 18:51:00,51, transfers: 1\n",
      "Itinerary number: 1, quality: 0.9975081657292914, dpt: Feb 04 2018 18:21:15,21, arr: Feb 04 2018 19:01:00,01, transfers: 1\n",
      "Itinerary number: 2, quality: 0.9932763142975513, dpt: Feb 04 2018 18:31:15,31, arr: Feb 04 2018 19:01:00,01, transfers: 1\n",
      "Itinerary number: 3, quality: 0.9975081657292914, dpt: Feb 04 2018 18:31:15,31, arr: Feb 04 2018 19:11:00,11, transfers: 1\n"
     ]
    }
   ],
   "source": [
    "## Print out the arrival time and quality of the three selected paths\n",
    "itinerary_selected_quality_ = [0,]*len(itinerary_acc)\n",
    "for i_ in range(len(itinerary_acc)):\n",
    "    itinerary_selected_quality_[i_] = comp_itinerary_quality(itinerary_acc[i_])\n",
    "    print('Fastest itineraries with quality constraint:')\n",
    "    print('Itinerary number: {}, quality: {}, dpt: {}, arr: {}, transfers: {}'.format(i_,itinerary_selected_quality_[i_], itinerary_acc[i_][0]['departure_time'], itinerary_acc[i_][-1]['arrival_time'],len(itinerary_acc[i_])-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Print news from SBB regarding the path\n",
    "date = [2018, Months_, Days_]\n",
    "test = display_info(date, [fromPlace_, toPlace_])\n",
    "for el in test: \n",
    "    print('\\n')\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Interface to be included above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quality_ = 0.90\n",
    "fromPlace_ = \"Zürich, Zürichbergstrasse\"\n",
    "toPlace_ = 'Zürich Enge, Bahnhof'\n",
    "Months_ = 2\n",
    "Days_ = 4\n",
    "Hours_ = 6\n",
    "AM_PM_ = 'PM'\n",
    "Minutes_ = 20\n",
    "Seconds_ = 1\n",
    "departure_ = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_itinerary_with_quality(fromPlace , toPlace, Months, Days, Hours, AM_PM, Minutes, Seconds, departure, quality):\n",
    "\n",
    "    #Get the json of quickest itineraries from local OTP server\n",
    "    test_json = return_request(fromPlace=fromPlace , toPlace= toPlace ,Months = Months, Days= Days, Hours= Hours, AM_PM = AM_PM, Minutes = Minutes, Seconds = Seconds, departure = departure)\n",
    "    #Create Dataframe to find LineId from ProductId, LineType and OperatorName. Relevant for Bus and Tram\n",
    "    #\n",
    "    #Read json and create itinerary list of dicts\n",
    "    itinerary_first_list = read_json_extract_itineraries(test_json, df_BT)\n",
    "    itinerary_acc, itinerary_refu = split_with_quality(itinerary_first_list, Quality = quality)\n",
    "    itinerary_searched = []\n",
    "    iter_=0\n",
    "\n",
    "    ## sort bad quality itineraries by arrival time\n",
    "    sorter_ids = np.argsort([itinerary_refu[i_][-1]['arrival_time'] for i_ in range(len(itinerary_refu))])\n",
    "    itinerary_refu = np.array(itinerary_refu)[sorter_ids].tolist()\n",
    "    while len(itinerary_refu) != 0:\n",
    "        if len(itinerary_acc)>=3:\n",
    "            break\n",
    "        iter_+=1\n",
    "        print('itineraries expanded: {}'.format(iter_))\n",
    "        itinerary_searched_ = itinerary_refu.pop(0)\n",
    "        itinerary_searched.append(itinerary_searched_)\n",
    "        #print(itinerary_searched_[0]['departure_time'], itinerary_searched_[-1]['arrival_time'])\n",
    "        #print(itinerary_searched_)\n",
    "        itinerary_test_list_explored = explore_itineraries(itinerary_searched_, df_BT, quality)\n",
    "        itinerary_acc_explored, itinerary_refu_explored = split_with_quality(itinerary_test_list_explored, quality)\n",
    "        for iti_refu in itinerary_refu_explored:\n",
    "            if not(any([(iti_refu == iti) for iti in itinerary_refu+itinerary_searched])):\n",
    "                itinerary_refu.append(iti_refu)\n",
    "        for iti_acc in itinerary_acc_explored:\n",
    "            if not(any([(iti_acc == iti) for iti in itinerary_acc])):\n",
    "                itinerary_acc.append(iti_acc)\n",
    "\n",
    "        ## sort by arrival time\n",
    "        sorter_ids = np.argsort([itinerary_refu[i_][-1]['arrival_time'] for i_ in range(len(itinerary_refu))])\n",
    "        itinerary_refu = np.array(itinerary_refu)[sorter_ids].tolist()\n",
    "\n",
    "\n",
    "    ## sort selected itineraries by arrival time\n",
    "    sorter_ids = np.argsort([itinerary_acc[i_][-1]['arrival_time'] for i_ in range(len(itinerary_acc))])\n",
    "    itinerary_acc = np.array(itinerary_acc)[sorter_ids].tolist()\n",
    "\n",
    "\n",
    "    print('\\n Fastest itineraries without quality constraint:')\n",
    "    ## Initial OTP output\n",
    "    ## Print out the arrival time and quality of the three select paths\n",
    "    itinerary_initial_quality_ = [0,]*len(itinerary_first_list)\n",
    "    for i_ in range(len(itinerary_first_list)):\n",
    "        itinerary_initial_quality_[i_] = comp_itinerary_quality(itinerary_first_list[i_])\n",
    "        print('Itinerary number: {}, quality: {}, dpt: {}, arr: {}, transfers: {}'.format(i_,itinerary_initial_quality_[i_], itinerary_first_list[i_][0]['departure_time'], itinerary_first_list[i_][-1]['arrival_time'],len(itinerary_first_list[i_])-1))\n",
    "    \n",
    "    print('\\n Fastest itineraries with quality constraint:')\n",
    "    ## Print out the arrival time and quality of the three selected paths\n",
    "    itinerary_selected_quality_ = [0,]*len(itinerary_acc)\n",
    "    for i_ in range(len(itinerary_acc)):\n",
    "        itinerary_selected_quality_[i_] = comp_itinerary_quality(itinerary_acc[i_])\n",
    "        print('Itinerary number: {}, quality: {}, dpt: {}, arr: {}, transfers: {}'.format(i_,itinerary_selected_quality_[i_], itinerary_acc[i_][0]['departure_time'], itinerary_acc[i_][-1]['arrival_time'],len(itinerary_acc[i_])-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arrival_time': 'Feb 04 2018 18:41:00,41',\n",
       " 'departure_time': 'Feb 04 2018 18:26:00,26',\n",
       " 'from': 'Zürich, Kirche Fluntern',\n",
       " 'lat_long_from': [47.3766006837906, 8.56023498977618],\n",
       " 'lat_long_to': [47.3641286895461, 8.53156974905593],\n",
       " 'line_id': '85:3849:005',\n",
       " 'product_id': 'TRAM',\n",
       " 'to': 'Zürich Enge, Bahnhof'}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itinerary_test_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itineraries expanded: 1\n",
      "itineraries expanded: 2\n",
      "\n",
      " Fastest itineraries without quality constraint:\n",
      "Itinerary number: 0, quality: 0.016150519696289896, dpt: Feb 04 2018 18:21:15,21, arr: Feb 04 2018 18:41:00,41, transfers: 1\n",
      "Itinerary number: 1, quality: 0.016150519696289896, dpt: Feb 04 2018 18:31:15,31, arr: Feb 04 2018 18:51:00,51, transfers: 1\n",
      "Itinerary number: 2, quality: 0.016150519696289896, dpt: Feb 04 2018 18:41:15,41, arr: Feb 04 2018 19:01:00,01, transfers: 1\n",
      "\n",
      " Fastest itineraries with quality constraint:\n",
      "Itinerary number: 0, quality: 0.9932763142975513, dpt: Feb 04 2018 18:21:15,21, arr: Feb 04 2018 18:51:00,51, transfers: 1\n",
      "Itinerary number: 1, quality: 0.9975081657292914, dpt: Feb 04 2018 18:21:15,21, arr: Feb 04 2018 19:01:00,01, transfers: 1\n",
      "Itinerary number: 2, quality: 0.9932763142975513, dpt: Feb 04 2018 18:31:15,31, arr: Feb 04 2018 19:01:00,01, transfers: 1\n",
      "Itinerary number: 3, quality: 0.9975081657292914, dpt: Feb 04 2018 18:31:15,31, arr: Feb 04 2018 19:11:00,11, transfers: 1\n"
     ]
    }
   ],
   "source": [
    "find_itinerary_with_quality(fromPlace_ , toPlace_, Months_, Days_, Hours_, AM_PM_, Minutes_, Seconds_, departure_, quality_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take WALK from stop Zürich, Zürichbergstrasse  at Feb 04 2018 18:21:15,21 to Zürich, Kirche Fluntern arriving at Feb 04 2018 18:26:00,26\n",
      "Take TRAM from Zürich, Kirche Fluntern at Feb 04 2018 18:36:00,36 to Zürich Enge, Bahnhof arriving at Feb 04 2018 18:51:00,51\n"
     ]
    }
   ],
   "source": [
    "for leg in itinerary_acc[0]:\n",
    "    print('Take {} from {} at {} to {} arriving at {}'.format(leg['product_id'],leg['from'], leg['departure_time'], leg['to'], leg['arrival_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Print out the arrival time and quality of the three selected paths\n",
    "    itinerary_selected_quality_ = [0,]*len(itinerary_acc)\n",
    "    for i_ in range(len(itinerary_acc)):\n",
    "        itinerary_selected_quality_[i_] = comp_itinerary_quality(itinerary_acc[i_])\n",
    "        print('Itinerary number: {}, quality: {}, dpt: {}, arr: {}, transfers: {}'.format(i_,itinerary_selected_quality_[i_], itinerary_acc[i_][0]['departure_time'], itinerary_acc[i_][-1]['arrival_time'],len(itinerary_acc[i_])-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prettytable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-bfc4cc4b6a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprettytable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrettyTable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prettytable'"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-170-34eafd9ad13a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-170-34eafd9ad13a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(/n'test')\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "['Red','Yellow','Green','Brown','Blue','Pink','Grey']\\n\",\n",
    "    \"nb_pix_t = PrettyTable()\\n\",\n",
    "    \"nb_pix_t.field_names = ['Color','Image 1', 'Image 2', 'Image 3', 'Image 4']\\n\",\n",
    "    \"for i_ in range(nb_of_colors):\\n\",\n",
    "    \"    nb_pix_t.add_row([color_labels[i_], pixels_per_color[0,i_],pixels_per_color[1,i_],pixels_per_color[2,i_],pixels_per_color[3,i_]])\\n\",\n",
    "    \"print (nb_pix_t)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Days = [i for i in range(1, 32)]\n",
    "Months = [i for i in range(1, 13)]\n",
    "Hours = [i for i in range(0, 13)]\n",
    "AM_PM = ['AM', 'PM']\n",
    "Minutes = [0, 15, 30, 45]\n",
    "Seconds = [0, 15, 30, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StopName = Df_meta.select('StopName_Meta').distinct().collect()\n",
    "StopName = [str(x).replace('\"', \"'\") for x in StopName]\n",
    "StopName = [str(x)[19:] for x in StopName]\n",
    "StopName = [str(x).split(\"')\")[0] for x in StopName]\n",
    "StopName = sorted(StopName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "interact_manual(return_request, fromPlace=StopName, toPlace= StopName,Months = Months, Days= Days, Hours= Hours, AM_PM = AM_PM,Minutes=Minutes, Seconds = Seconds ,departure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_list = return_read_json(test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test to see what we have in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we don't have data for bus and subway, at least near Lausanne \n",
    "\n",
    "After investigation it's seems that we have data for the LEB in Lausanne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp.select('ProductId').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_tmp.filter(df_tmp['StopName'].rlike(\"Lausanne\") & (col('ProductId') != 'Zug')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp.where((col('ProductId') == 'Zug') & (col('Line_ID') == '108')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where((col('ProductId') == 'BUS')).select('BPUIC').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LineType_temp = itinerary_test_list[0][1]['f_id']\n",
    "StopName_temp = itinerary_test_list[0][1]['to']\n",
    "df.where((col('LineType') == LineType_temp)).where(col('StopName') == StopName_temp).select('LineId').head(2)[0].asDict()['LineId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(df.where((col('LineType') == LineType_temp)).where(col('StopName') == StopName_temp).select('LineId').head(2)[1].asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.where((col('ProductId') == 'BUS')).select('LineId','LineType','StopName').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list((df.where((col('ProductId') == 'Zug')).select('TripId','LineId','LineType').where(df['LineId']==2538).head(1)[0]).asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-2c5e395f4a5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_tmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1018\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1020\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1021\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "df_tmp.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tmp.where(df_tmp['StopName'].rlike(\"Lausanne\") &(col('ProductId') == 'Bus')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json(json_data):\n",
    "    for route in json_data['plan']['itineraries']:\n",
    "        #Here we show the 3 different path\n",
    "        print('-------- Route---------\\n')\n",
    "        #print(route)\n",
    "        for step in route['legs']: \n",
    "            #Here we show all the step of the route   \n",
    "            print('---Step---\\n')\n",
    "            #print(step)\n",
    "            from_ = step['from']['name']\n",
    "            to_ = step['to']['name']\n",
    "            mode = step['mode']\n",
    "            if('tripShortName' in step.keys()):\n",
    "                route_id = step['tripShortName']\n",
    "                #print(step)\n",
    "            end_time = str(step['endTime'])\n",
    "            duration = str(step['duration'])\n",
    "            print('For the travel from {} to {} in {} \\n'.format(from_, to_, mode))\n",
    "            print('The duration is {} and the arrival time is {}\\n'.format(time.strftime(\"%H:%M:%S\", time.gmtime(float(duration))), time.strftime(\"%b %d %Y %H:%M:%S,%M\", time.gmtime(float(end_time[:len(end_time)-3])))))\n",
    "            if('tripShortName' in step.keys()):\n",
    "                print('The route ID is {}'.format(route_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request(fromPlace, toPlace, departure,Hours, AM_PM,Minutes, Months, Days):\n",
    "    lat_from, long_from = get_lat_long(fromPlace)\n",
    "    lat_to, long_to = get_lat_long(toPlace)\n",
    "    url = 'http://10.90.38.21:8829/otp/routers/default/plan?fromPlace=stop+'\n",
    "    url += '+'.join(fromPlace.split()) +  '+%3A%3A' + str(lat_from) + '%2C' + str(long_from)\n",
    "    url += '&toPlace=stop+' +  '+'.join(toPlace.split()) +  '+%3A%3A' + str(lat_to) + '%2C' + str(long_to)\n",
    "    url += '&time={}%3A{}{}&date={}-{}-2018&mode=TRANSIT%2CWALK&maxWalkDistance=804.672&arriveBy=true&wheelchair=false&locale=en&numItineraries=6'.format(Hours, Minutes,AM_PM, Months, Days)\n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    print(r)\n",
    "    #print(r.json())\n",
    "    read_json(r.json())\n",
    "    #return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_itineraries_tree(itinerary, df_BT):\n",
    "    itinerary_list = []\n",
    "    for j_ in range(len(itinerary)-1):\n",
    "        leg_1 = itinerary[j_]\n",
    "        arr_month, arr_day, arr_hour, arr_minute, arr_AM_PM, arr_second = date_to_cells(leg_1['arrival_time'])\n",
    "        #new_semi_its = request_with_quality(fromPlace = leg_1['to'], toPlace = itinerary[-1]['to'], Months = arr_month, Days = arr_day, Hours = arr_hour, Minutes = arr_minute, Seconds = arr_second, AM_PM = arr_AM_PM, departure = True, Quality = quality, lat_long_from = leg_1['lat_long_from'], lat_long_to = itinerary[-1]['lat_long_to'] )\n",
    "        temp_json = return_request(fromPlace = leg_1['to'], toPlace = itinerary[-1]['to'], Months = arr_month, Days = arr_day, Hours = arr_hour, Minutes = arr_minute, Seconds = arr_second, AM_PM = arr_AM_PM, departure = True, lat_long_from = leg_1['lat_long_to'], lat_long_to = itinerary[-1]['lat_long_to'])\n",
    "        #print(temp_json)\n",
    "        new_partial_its = read_json_extract_itineraries(temp_json, df_BT)\n",
    "        it_list = []\n",
    "        for k_ in range(len(new_partial_its)):\n",
    "            if len(new_partial_its[k_]) == 1:\n",
    "                it_list.extend(new_partial_its[k_])\n",
    "            if len(new_partial_its[k_]) != 1:\n",
    "                it_list.extend(explore_itineraries(new_partial_its[k_], df_BT))\n",
    "        \n",
    "        new_itineraries = [np.append(itinerary[:j_+1],it_list[m_]).tolist() for m_ in range(len(it_list))]\n",
    "        \n",
    "        itinerary_list.extend(new_itineraries)\n",
    "    return itinerary_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
