{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "plt.rcParams['figure.figsize'] = (10,6)\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.90.38.21:4055\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0.2.6.4.0-91</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>final_proj-baetu</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=final_proj-baetu>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = pyspark.conf.SparkConf()\n",
    "conf.setMaster('yarn')\n",
    "conf.setAppName('final_proj-{0}'.format(getpass.getuser()))\n",
    "conf.set('spark.executor.memory', '4g')\n",
    "conf.set('spark.executor.instances', '10')\n",
    "conf.set('spark.port.maxRetries', '100')\n",
    "sc = pyspark.SparkContext.getOrCreate(conf)\n",
    "conf = sc.getConf()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/datasets/project/istdaten/*/*/*', sep=';', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we rename the columns in English language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = 'TripDate string, TripId string, OperatorId string, OperatorAbbrv string, OperatorName string, ProductId string, LineId string, LineType string, UmlaufId string, TransportType string, AdditionalTrip boolean, FailedTrip boolean, BPUIC string, StopName string, ArrivalTimeScheduled string, ArrivalTimeActual string, ArrivalTimeActualStatus string,     DepartureTimeScheduled string, DepartureTimeActual string, DepartureTimeActualStatus string, SkipStation boolean'\n",
    "columns = list(map(lambda x: x.split()[0],columns.split(',')))\n",
    "\n",
    "for old, new in zip(df.columns, columns):\n",
    "    df = df.withColumnRenamed(old, new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the quality of a transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions: \n",
    "\n",
    "   * everytime when making a transfer in a station, the traveler needs one minute for actually changing transport.\n",
    "   * even though a train departs late all the time in a specific station, the trip planner will never use the fact that it does so, so we will only take into consideration the early departures and the correct ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea: \n",
    "\n",
    "The idea behind computing the quality of a specific transfer given the *expected arrival hour* in the station and the *expected departure hour* from that same station, and some *extra information* regarding the trip before the transfer and the one after the transfer:\n",
    "\n",
    "   * First, we compute the **discrete distribution of arrival delays $\\mathcal{D}_a$** in that station, given the information of the trip before the transfer.\n",
    "   * Then, we compute the **discrete distribution of negative departure delays $\\mathcal{D}_d$** in that station, given the information of the trip after the transfer.\n",
    "   * Next, we compute the probability of successfully realizing the transfer, by computing a convolution between the two given distributions. Therefore, assuming that the time of transfer is $k$ minutes, then we would simply compute:\n",
    "      \n",
    "      $\\sum\\limits_{t_a }\\Pr[\\mathcal{D}_a = t_a] \\cdot \\Pr[\\mathcal{D}_d = k-1+t_a]$,\n",
    "      \n",
    "       where we have taken into consideration the minute needed by the traveler for changing the transport. \n",
    "       \n",
    "---\n",
    "       \n",
    "Therefore, we first need to decide what are the features which will decide the distributions of the delays. For that, we will use a **Decision Tree Regressor**, selecting several features which might be important from the data, and the target label will be the delay for each datapoint, expressed in seconds. Then, we will train the regressor on both departures and arrivals data, and will look into which are the most important features in each case, for making a good prediction of the delay time. \n",
    "\n",
    "We have to emphasize that we considered this method, because of the way that Decision Trees decide which are the most important feature, i.e. the one which have the most variance of delays between the different values for the specific feature. \n",
    "\n",
    "After constructing the Decision Tree and deciding which are the most important features, we will construct the distributions of the delays from the **actual data**, by grouping the datapoints with the same value for the decisive features, and making the distribution of delays for each group.\n",
    "\n",
    "We decided to use the actual data instead of modelling the distribution of delays using a fixed distribution family (e.g. Log-normal or Gamma distributions), because we consider that the actual data is more relevant, then considering just an estimator or to assume that it follows a distribution in a family of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Constructing the Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in constructing the Decision Tree Regressor is to construct some potential important features from the given data, and also to compute the delays for each datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, to_timestamp\n",
    "\n",
    "DATE_FORMAT_SCHEDULED = 'dd.MM.yyyy HH:mm' \n",
    "DATE_FORMAT_ACTUAL = 'dd.MM.yyyy HH:mm:ss' # both formats are used\n",
    "\n",
    "df_processed = df.withColumn('ArrivalTimeScheduledDate', to_timestamp(df.ArrivalTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "df_processed = df_processed.withColumn('DepartureTimeScheduledDate', to_timestamp(df_processed.DepartureTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "\n",
    "df_processed = df_processed.withColumn('ArrivalTimeScheduled', unix_timestamp(df_processed.ArrivalTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "df_processed = df_processed.withColumn('ArrivalTimeActual', unix_timestamp(df_processed.ArrivalTimeActual, DATE_FORMAT_ACTUAL))\n",
    "df_processed = df_processed.withColumn('DepartureTimeScheduled', unix_timestamp(df_processed.DepartureTimeScheduled, DATE_FORMAT_SCHEDULED))\n",
    "df_processed = df_processed.withColumn('DepartureTimeActual', unix_timestamp(df_processed.DepartureTimeActual, DATE_FORMAT_ACTUAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into how the data looks so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(TripDate='13.09.2017', TripId='80:06____:17010:000', OperatorId='80:06____', OperatorAbbrv='DB', OperatorName='DB Regio AG', ProductId='Zug', LineId='17010', LineType='RE', UmlaufId=None, TransportType='RE', AdditionalTrip='false', FailedTrip='false', BPUIC='8500090', StopName='Basel Bad Bf', ArrivalTimeScheduled=None, ArrivalTimeActual=None, ArrivalTimeActualStatus='PROGNOSE', DepartureTimeScheduled=1505274300, DepartureTimeActual=1505274300, DepartureTimeActualStatus='PROGNOSE', SkipStation='false', ArrivalTimeScheduledDate=None, DepartureTimeScheduledDate=datetime.datetime(2017, 9, 13, 5, 45))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also add the hour of departure and of the arrival to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[line_id: string, product_id: string, stop_name: string, additional_trip: string, arrival_hour: string, departure_hour: string, day_of_week: string, delta_arrival: float, delta_departure: float]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import FloatType, StringType\n",
    "from pyspark.sql.functions import hour, to_date, date_format, month\n",
    "\n",
    "df_to_classify = df_processed.select(\n",
    "    df_processed.LineId.alias('line_id'), \n",
    "    df_processed.ProductId.alias('product_id'), \n",
    "    df_processed.StopName.alias('stop_name'),\n",
    "    df_processed.AdditionalTrip.alias('additional_trip'), \n",
    "    hour(df_processed.ArrivalTimeScheduledDate).alias(\"arrival_hour\").astype(StringType()),\n",
    "    hour(df_processed.DepartureTimeScheduledDate).alias(\"departure_hour\").astype(StringType()),\n",
    "    date_format(to_date(df_processed.TripDate, 'dd.MM.yyyy'), 'u').alias(\"day_of_week\"),\n",
    "    ((df_processed.ArrivalTimeActual - df_processed.ArrivalTimeScheduled)).alias(\"delta_arrival\").astype(FloatType()),\n",
    "    ((df_processed.DepartureTimeActual - df_processed.DepartureTimeScheduled)).alias(\"delta_departure\").astype(FloatType()))\n",
    "\n",
    "df_to_classify.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(line_id='17010', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='5', day_of_week='3', delta_arrival=None, delta_departure=0.0),\n",
       " Row(line_id='17012', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='6', day_of_week='3', delta_arrival=None, delta_departure=0.0),\n",
       " Row(line_id='17013', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour='6', departure_hour=None, day_of_week='3', delta_arrival=180.0, delta_departure=None),\n",
       " Row(line_id='17014', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='9', day_of_week='3', delta_arrival=None, delta_departure=0.0),\n",
       " Row(line_id='17015', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour='8', departure_hour=None, day_of_week='3', delta_arrival=300.0, delta_departure=None),\n",
       " Row(line_id='17016', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='10', day_of_week='3', delta_arrival=None, delta_departure=0.0),\n",
       " Row(line_id='17017', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour='10', departure_hour=None, day_of_week='3', delta_arrival=60.0, delta_departure=None),\n",
       " Row(line_id='17018', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='11', day_of_week='3', delta_arrival=None, delta_departure=780.0),\n",
       " Row(line_id='17019', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour='11', departure_hour=None, day_of_week='3', delta_arrival=300.0, delta_departure=None),\n",
       " Row(line_id='17020', product_id='Zug', stop_name='Basel Bad Bf', additional_trip='false', arrival_hour=None, departure_hour='12', day_of_week='3', delta_arrival=None, delta_departure=180.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_classify.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for using the Decision Tree Regressor, and because each feature is in fact categorial, we must each one of them using a *StringIndexer*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def transform_dataset(dataset, departure):\n",
    "    '''\n",
    "    Function that transforms a dataset, adding for each categorial feature a column, which represents the output of the \n",
    "    StringIndexer applied to that column. \n",
    "    \n",
    "    Parameters:\n",
    "        - dataset: the dataset to be processed\n",
    "        - departure: True if the dataset is for departures, False otherwise\n",
    "    '''\n",
    "    \n",
    "    line_id_indexer = StringIndexer(inputCol=\"line_id\", outputCol=\"line_id_cat\", handleInvalid='keep') # keep nulls \n",
    "    product_id_indexer = StringIndexer(inputCol=\"product_id\", outputCol=\"product_id_cat\", handleInvalid='skip')\n",
    "    stop_name_indexer = StringIndexer(inputCol=\"stop_name\", outputCol=\"stop_name_cat\", handleInvalid='skip')\n",
    "    additional_trip_indexer = StringIndexer(inputCol=\"additional_trip\", outputCol=\"additional_trip_cat\", handleInvalid='skip')\n",
    "    day_of_week_indexer = StringIndexer(inputCol=\"day_of_week\", outputCol=\"day_of_week_cat\", handleInvalid='skip')\n",
    "    departure_hour_indexer = StringIndexer(inputCol=\"departure_hour\", outputCol=\"departure_hour_cat\", handleInvalid='skip')\n",
    "    arrival_hour_indexer = StringIndexer(inputCol=\"arrival_hour\", outputCol=\"arrival_hour_cat\", handleInvalid='skip')\n",
    "\n",
    "    indexers = [line_id_indexer, product_id_indexer, stop_name_indexer, additional_trip_indexer,day_of_week_indexer]\n",
    "    \n",
    "    if departure:\n",
    "        indexers.append(departure_hour_indexer)\n",
    "    else:\n",
    "        indexers.append(arrival_hour_indexer)\n",
    "\n",
    "    indexed = dataset\n",
    "\n",
    "    for indexer in indexers:\n",
    "        indexed = indexer.fit(indexed).transform(indexed) # add columns to dataset\n",
    "        \n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the *VectorAssembler* to construct the column for features, which will be used by the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "def compute_features_column(dataset, is_departure):\n",
    "    '''\n",
    "    Function that computes the features column for the given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        - dataset: the dataset to compute the features column for\n",
    "        - is_departure: True is dataset is used for departures, False otherwise.\n",
    "    '''\n",
    "    input_cols = ['line_id_cat', 'product_id_cat', 'stop_name_cat', 'additional_trip_cat', 'day_of_week_cat']\n",
    "    \n",
    "    if is_departure:\n",
    "        input_cols.append('departure_hour_cat') # departure dataset\n",
    "    else:\n",
    "        input_cols.append('arrival_hour_cat') # arrival dataset\n",
    "        \n",
    "    vector_assembler = VectorAssembler(inputCols = input_cols, outputCol = 'features')\n",
    "    dataset = transform_dataset(dataset, is_departure) # add categorial features\n",
    "    \n",
    "    df_features = vector_assembler.transform(dataset) # add features column\n",
    "    # Use VectorIndexer to make sure that the added features are recognized as categorical\n",
    "    \n",
    "    featureIndexer = \\\n",
    "        VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=100000000).fit(df_features)\n",
    "    \n",
    "    df_features = featureIndexer.transform(df_features) # transform features to categorical\n",
    "    \n",
    "    if is_departure:\n",
    "        df_final = df_features.select(df_features.indexedFeatures, df_features.delta_departure.alias(\"delta\"))\n",
    "    else:\n",
    "        df_final = df_features.select(df_features.indexedFeatures, df_features.delta_arrival.alias(\"delta\"))\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct our datasets to input to the Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct departures dataset\n",
    "df_departure_to_regress = df_to_classify.filter(\n",
    "    df_to_classify.departure_hour.isNotNull() & # filter only departures\n",
    "    df_to_classify.delta_departure.isNotNull())\n",
    "\n",
    "df_departure = compute_features_column(df_departure_to_regress, is_departure=True)\n",
    "\n",
    "# Construct arrivals dataset\n",
    "df_arrival_to_regress = df_to_classify.filter(\n",
    "    df_to_classify.arrival_hour.isNotNull() & # filter only arrivals\n",
    "    df_to_classify.delta_arrival.isNotNull())\n",
    "\n",
    "df_arrival = compute_features_column(df_arrival_to_regress, is_departure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the generated dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(indexedFeatures=DenseVector([14483.0, 2.0, 2333.0, 0.0, 2.0, 18.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([10292.0, 2.0, 2333.0, 0.0, 2.0, 11.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([10275.0, 2.0, 2333.0, 0.0, 2.0, 13.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([13971.0, 2.0, 2333.0, 0.0, 2.0, 12.0]), delta=0.0),\n",
       " Row(indexedFeatures=DenseVector([14059.0, 2.0, 2333.0, 0.0, 2.0, 8.0]), delta=780.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_departure.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(indexedFeatures=DenseVector([10223.0, 2.0, 2006.0, 0.0, 2.0, 11.0]), delta=180.0),\n",
       " Row(indexedFeatures=DenseVector([9633.0, 2.0, 2006.0, 0.0, 2.0, 4.0]), delta=300.0),\n",
       " Row(indexedFeatures=DenseVector([12909.0, 2.0, 2006.0, 0.0, 2.0, 12.0]), delta=60.0),\n",
       " Row(indexedFeatures=DenseVector([12924.0, 2.0, 2006.0, 0.0, 2.0, 8.0]), delta=300.0),\n",
       " Row(indexedFeatures=DenseVector([10172.0, 2.0, 2006.0, 0.0, 2.0, 6.0]), delta=300.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arrival.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the function for training the Decision Tree Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "def train_regressor(dataset):\n",
    "    dt = DecisionTreeRegressor(featuresCol ='indexedFeatures', labelCol = 'delta', maxBins=100000000, maxDepth=3)\n",
    "    dt_model = dt.fit(dataset)\n",
    "    \n",
    "    return dt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the decision trees for both datasets and we extract the most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importances departures: (6,[0,2,5],[0.284510290083,0.0974031881251,0.618086521792])\n",
      "Feature importances arrivals: (6,[0,2,3,5],[0.334593468,0.0606024087098,0.0354578885646,0.569346234726])\n"
     ]
    }
   ],
   "source": [
    "# Get most important features for departures dataset\n",
    "regressor_departures = train_regressor(df_departure)\n",
    "print(\"Feature importances departures: {}\".format(regressor_departures.featureImportances))\n",
    "\n",
    "# Get most important features for arrivals dataset\n",
    "regressor_arrivals = train_regressor(df_arrival)\n",
    "print(\"Feature importances arrivals: {}\".format(regressor_arrivals.featureImportances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the 3 most important features are, in both cases, the *hour*, the *line_id* and the *stop_name*. We can see that everything makes very much sense, because we have big differences of delays between normal hours and rush hours, for example, and also specific stops and routes have usually more delays than the others.\n",
    "\n",
    "Therefore, we continue by constructing the probability distributions for each possible value of the three most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the probability distributions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we only consider the three most important features in the two initial datasets. We will consider the unity of time to be the minute from now on, instead of seconds: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df_best_feat_departures = df_departure_to_regress.select(\n",
    "                df_departure_to_regress.departure_hour,\n",
    "                df_departure_to_regress.stop_name,\n",
    "                df_departure_to_regress.line_id,\n",
    "                (df_departure_to_regress.delta_departure / 60).astype(IntegerType()).alias(\"delta_minutes\"))\n",
    "\n",
    "df_best_feat_departures = df_best_feat_departures.filter(df_best_feat_departures.delta_minutes <= 0) \n",
    "# only keep departures which left on time or earlier, we do not want to base our recommendation on assumption\n",
    "# that a train or bus leaves with a delay.\n",
    "\n",
    "df_best_feat_arrival = df_arrival_to_regress.select(\n",
    "                df_arrival_to_regress.arrival_hour,\n",
    "                df_arrival_to_regress.stop_name,\n",
    "                df_arrival_to_regress.line_id,\n",
    "                (df_arrival_to_regress.delta_arrival / 60).astype(IntegerType()).alias(\"delta_minutes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(departure_hour='5', stop_name='Basel Bad Bf', line_id='17010', delta_minutes=0),\n",
       " Row(departure_hour='6', stop_name='Basel Bad Bf', line_id='17012', delta_minutes=0),\n",
       " Row(departure_hour='9', stop_name='Basel Bad Bf', line_id='17014', delta_minutes=0),\n",
       " Row(departure_hour='10', stop_name='Basel Bad Bf', line_id='17016', delta_minutes=0),\n",
       " Row(departure_hour='14', stop_name='Basel Bad Bf', line_id='17024', delta_minutes=0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_best_feat_departures.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to make the distribution of delays for each possible value of the features, for both departures and arrivals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list, struct, count, lit\n",
    "\n",
    "df_departures_grouped_count = df_best_feat_departures.groupby( \n",
    "                df_best_feat_departures.departure_hour,\n",
    "                df_best_feat_departures.stop_name,\n",
    "                df_best_feat_departures.line_id,\n",
    "                df_best_feat_departures.delta_minutes).agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "        \n",
    "df_departures_distribution = df_departures_grouped_count.\\\n",
    "                                    groupby('departure_hour', 'stop_name', 'line_id').\\\n",
    "                                    agg(collect_list(struct('delta_minutes', 'count_min')).alias('counts'))\n",
    "\n",
    "# for each value of (departure_hour, stop_name, line_id), we have a list of the form [(delay_minutes, count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(departure_hour='13', stop_name='Payerne', line_id='12950', delta_minutes=0, count_min=164),\n",
       " Row(departure_hour='15', stop_name='Lausanne', line_id='12955', delta_minutes=0, count_min=141),\n",
       " Row(departure_hour='7', stop_name='Yverdon-les-Bains', line_id='14518', delta_minutes=0, count_min=137)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_departures_grouped_count.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-------+--------+\n",
      "|departure_hour|  stop_name|line_id|  counts|\n",
      "+--------------+-----------+-------+--------+\n",
      "|             0|        Bex|   3591|[[0,50]]|\n",
      "|             0|Biel/Bienne|   7845|[[0,65]]|\n",
      "+--------------+-----------+-------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_departures_distribution.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_key_for_feature_values(hour, line_id, stop_name):\n",
    "    return '{}#{}#{}'.format(hour, line_id, stop_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collected = df_departures_distribution.collect()\n",
    "\n",
    "distribution_departures = {\n",
    "    compute_key_for_feature_values(x.departure_hour, x.line_id, x.stop_name) : \n",
    "    list(sorted(x.counts, key=lambda y: y[0])) for x in collected}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same now for the arrivals: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_arrivals_grouped_count = df_best_feat_arrival.groupby( \n",
    "                df_best_feat_arrival.arrival_hour,\n",
    "                df_best_feat_arrival.stop_name,\n",
    "                df_best_feat_arrival.line_id,\n",
    "                df_best_feat_arrival.delta_minutes).agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "        \n",
    "df_arrivals_distribution = df_arrivals_grouped_count.\\\n",
    "                                    groupby('arrival_hour', 'stop_name', 'line_id').\\\n",
    "                                    agg(collect_list(struct('delta_minutes', 'count_min')).alias('counts'))\n",
    "        \n",
    "collected = df_arrivals_distribution.collect()\n",
    "\n",
    "distribution_arrivals = {\n",
    "    compute_key_for_feature_values(x.arrival_hour, x.line_id, x.stop_name) : \n",
    "    list(sorted(x.counts, key=lambda y: y[0])) for x in collected}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to include a default distribution, for the case we have new data, which was not encountered anymore. We will compute it as the distribution of all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_default_distrib_departures = df_best_feat_departures.groupby('delta_minutes').agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "collected_default = df_default_distrib_departures.collect()\n",
    "default_departures = list(sorted(collected_default, key=lambda x: x[0]))\n",
    "\n",
    "df_default_distrib_arrivals = df_best_feat_arrival.groupby('delta_minutes').agg(count(lit(1)).alias(\"count_min\")) # add a count for each possible value\n",
    "collected_default = df_default_distrib_arrivals.collect()\n",
    "default_arrivals = list(sorted(collected_default, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add the default values to the dictionary of distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution_departures['default'] = default_departures\n",
    "distribution_arrivals['default'] = default_arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the counts to probabilities, to be able to compute the final quality faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_proba(counts_list):\n",
    "    total_sum = 0\n",
    "    final_proba = []\n",
    "    \n",
    "    for row in counts_list:\n",
    "        total_sum += row.count_min\n",
    "        \n",
    "    for row in counts_list:\n",
    "        final_proba.append((row.delta_minutes, row.count_min / total_sum))\n",
    "        \n",
    "    return final_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distribution_departures = {k : transform_to_proba(v) for k, v in distribution_departures.items()}\n",
    "distribution_arrivals = {k : transform_to_proba(v) for k, v in distribution_arrivals.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally write the computed dictionaries to file, to be able to load them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "FILE_DISTRIBUTION_DEPARTURES = 'distrib_departures.pic'\n",
    "FILE_DISTRIBUTION_ARRIVALS = 'distrib_arrivals.pic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(distribution_departures, open(FILE_DISTRIBUTION_DEPARTURES, 'wb'))\n",
    "pickle.dump(distribution_arrivals, open(FILE_DISTRIBUTION_ARRIVALS, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exposed API for computing distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last part is to write a function which receives the features of a specific transfer, and it returns the quality of the transfer, by performing the convolution of the corresponding distributions, using the formula:\n",
    "\n",
    "$\\sum\\limits_{t_a }\\Pr[\\mathcal{D}_a = t_a] \\cdot \\Pr[\\mathcal{D}_d = k-1+t_a]$,\n",
    "      \n",
    "where we have taken into consideration the minute needed by the traveler for changing the transport. \n",
    "       \n",
    "Here, we considered $\\mathcal{D}_a$ to be the distribution of arrivals and $\\mathcal{D}_d$ the distribution of departures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "DATE_FORMAT = '%b %d %Y %H:%M:%S'\n",
    "\n",
    "class TransferQualityComputer:\n",
    "    def __init__(self):\n",
    "        self.distribution_departures = pickle.load(open(FILE_DISTRIBUTION_DEPARTURES, 'rb'))\n",
    "        self.distribution_arrivals = pickle.load(open(FILE_DISTRIBUTION_ARRIVALS, 'rb'))\n",
    "        \n",
    "    def compute_key_for_feature_values(self, hour, line_id, stop_name): # same as before\n",
    "        return '{}#{}#{}'.format(hour, line_id, stop_name)\n",
    "    \n",
    "    def compute_quality(self, arrival_timestamp, departure_timestamp, stop_name, line_id):\n",
    "        # timestamp in the format: Dec 31 2017 20:40:49,01\n",
    "\n",
    "        arrival_time = datetime.strptime(arrival_timestamp[:-3], DATE_FORMAT)\n",
    "        departure_time = datetime.strptime(departure_timestamp[:-3], DATE_FORMAT)\n",
    "\n",
    "        arrival_hour = arrival_time.hour\n",
    "        departure_hour = departure_time.hour\n",
    "        delta_minutes = int((time.mktime(departure_time.timetuple()) - time.mktime(arrival_time.timetuple())) / 60)\n",
    "\n",
    "        if delta_minutes < 0:\n",
    "            return 0 # impossible to complete the transfer\n",
    "\n",
    "        departure_key = self.compute_key_for_feature_values(departure_hour, line_id, stop_name)\n",
    "        if departure_key in self.distribution_departures:\n",
    "            departure_dist = self.distribution_departures[departure_key]\n",
    "        else: \n",
    "            departure_dist = self.distribution_departures['default'] # default distribution\n",
    "\n",
    "        arrival_key = self.compute_key_for_feature_values(arrival_hour, line_id, stop_name)\n",
    "        if arrival_key in self.distribution_arrivals:\n",
    "            arrival_dist = self.distribution_arrivals[arrival_key]\n",
    "        else: \n",
    "            arrival_dist = self.distribution_arrivals['default'] # default distribution\n",
    "\n",
    "        total_proba = 0\n",
    "\n",
    "        for dep_delay, dep_proba in departure_dist:\n",
    "            for arr_delay, arr_proba in arrival_dist:\n",
    "\n",
    "                delta_minutes = ((departure_time - arrival_time).seconds // 60) % 60\n",
    "                if delta_minutes >= dep_delay + arr_delay + 1:\n",
    "                    total_proba += (dep_proba * arr_proba)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return total_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8011350894443582\n"
     ]
    }
   ],
   "source": [
    "computer = TransferQualityComputer()\n",
    "\n",
    "print(computer.compute_quality('Dec 31 2017 00:40:49,01', 'Dec 31 2017 00:41:58,01', 'Dietikon, Birmensdorferstrasse', '85:849:303'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
